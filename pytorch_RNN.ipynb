{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Music Generation with RNNs\n",
    "\n",
    "Inspired by MIT Deep Learning Tensorflow class. This project uses Pytorch and Recurrent Neural Network (RNN) for music generation. Training set is a small dataset of MIDI format pop song snippets. It's very easy to generate new snippets and convert to MIDI format so this training set can easily expand. You can even train it with a specific genre of songs by only supplying it with data from that genre. Libraries used: python-midi, Pytorch, MIT custom create_dataset.py and midi_manipulation.py (see the util folders). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-midi in /Users/ysun/anaconda2/lib/python2.7/site-packages (0.2.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install python-midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from util.util import print_progress\n",
    "from util.create_dataset import create_dataset, get_batch\n",
    "from util.midi_manipulation import noteStateMatrixToMidi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "Data is stored in the data folder. Data is pre converted to `np.array` notes are one hot encoded. Only keep songs with length above min_song_length threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 songs processed\n",
      "15 songs discarded\n"
     ]
    }
   ],
   "source": [
    "min_song_length  = 128\n",
    "encoded_songs    = create_dataset(min_song_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 total songs to learn from\n",
      "(129, 78)\n"
     ]
    }
   ],
   "source": [
    "NUM_SONGS = len(encoded_songs)\n",
    "print(str(NUM_SONGS) + \" total songs to learn from\")\n",
    "print(encoded_songs[0].shape) #(song_length, num_possible_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model Architecture\n",
    "\n",
    "Cell type is LSTM Cell. \"This model will be based off a single LSTM cell, with a state vector used to maintain temporal dependencies between consecutive music notes.\" (same model as MIT Deep Learning 191). The model takes a sequence of input of previous note, put it through an LSTM cell, then into a fully connected layer. Its output is fed through a softmax function which returns a probability distribution over the next note. \n",
    "\n",
    "$$ P(x_t\\vert x_{t-L},\\cdots,x_{t-1})$$ \n",
    "\n",
    "where $x_t$ is a one-hot encoding of the note played at timestep $t$ and $L$ is the length of a song snippet, as shown in the diagram below.\n",
    "\n",
    "<img src=\"img/lab1ngram.png\" alt=\"Drawing\" style=\"width: 50em;\"/>\n",
    " \n",
    "\n",
    "### Neural Network Parameters\n",
    "* `input_size` and `output_size` are defined to match the shape of the **encoded** inputs and outputs at each timestep. Recall that the encoded representation of each song  has shape `(song_length, num_possible_notes)`, with the notes played at each timestep encoded as a binary vector over all possible notes. The parameters `input_size` and `output_size` will reflect the length of this vector encoding -- the number of possible notes.\n",
    "* `hidden_size` is the number of states in  LSTM and the size of the hidden layer after our LSTM.\n",
    "* The `learning_rate` of the model should be somewhere between `1e-4` and `0.1`. \n",
    "* `training_steps` is the number of batches. \n",
    "* The `batch_size` is the number of song snippets per batch.\n",
    "* To train the model, we will be choosing snippets of length `timesteps` from each song. This ensures that all song snippets have the same length and speeds up training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neural Network Parameters\n",
    "input_size       = encoded_songs[0].shape[1]   # The number of possible MIDI Notes\n",
    "output_size      = input_size                  # Same as input size\n",
    "hidden_size      = 128                         # Number of neurons in hidden layer\n",
    "\n",
    "learning_rate    = 0.001 # Learning rate of the model\n",
    "training_steps   = 200  # Number of batches during training\n",
    "batch_size       = 256    # Number of songs per batch\n",
    "timesteps        = 64    # Length of song snippet -- this is what is fed into the model\n",
    "\n",
    "assert timesteps < min_song_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization\n",
    "\n",
    "### Dimensions\n",
    "\n",
    "* Input size  is 3 dimensional: size of the batch, number of time steps in a song snippet, number of possible MIDI notes\n",
    "* Output size is 2 dimensional: it is just the single note that immediately follows a song snippet in the input tensor for each song snippet in the training batch. Size of the batch, the number of possible MIDI notes.\n",
    "\n",
    "Why all possible MIDI notes? Because of the hot encoding, each note has a binary field either 0 or 1, but there can be only one 1 in each row. It's unique. \n",
    "\n",
    "There's a fully connected layer after the LSTM with weights and biases. It's easy to replace this last layer using Pytorch nn.Sequential with a named layer. \n",
    "\n",
    "In Tensorflow need to use `tf.placeholder` and also mind the shape. In Pytorch, todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIMENSIONS\n",
    "# todo input batch_size, timesteps, input_size\n",
    "# todo output batch_size, outputsize\n",
    "# FC DIMENSIONS\n",
    "# todo weights hidden_size, output_size\n",
    "# todo biases output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pytorch it is easy to setup RNN with LSTM cells. [See docs](https://pytorch.org/docs/stable/nn.html#lstm)\n",
    "In Tensorflow use `RNN(input_vec, weights, biases)`  `rnn.BasicLTMCell` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIMENSIONS\n",
    "# todo input batch_size, timesteps, input_size\n",
    "# todo output batch_size, outputsize\n",
    "# FC DIMENSIONS\n",
    "# todo weights hidden_size, output_size\n",
    "# todo biases output_size\n",
    "\n",
    "class MusicRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=hidden_size, n_layers=1, lr=learning_rate):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        ## TODO: define the LSTM\n",
    "        self.lstm = nn.LSTM(batch_size, n_hidden, n_layers,batch_first=True)\n",
    "        \n",
    "        ## TODO: define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, output_size)\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        ## TODO: pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss, Training, and Accuracy Operations\n",
    "For training we use softmax cross entropy loss as the criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, prediction = RNN(input_vec, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS OPERATION:\n",
    "'''TODO: Use TensorFlow to define the loss operation as the mean softmax cross entropy loss. \n",
    "TensorFlow has built-in functions for you to use. '''\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=logits, labels=output_vec))  # TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING OPERATION:\n",
    "'''TODO: Define an optimizer for the training operation. \n",
    "Remember we have already set the `learning_rate` parameter.'''\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) # TODO\n",
    "train_op = optimizer.minimize(loss_op) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCURACY: We compute the accuracy in two steps.\n",
    "\n",
    "# First, we need to determine the predicted next note and the true next note, across the training batch, \n",
    "#  and then determine whether our prediction was correct. \n",
    "# Recall that we defined the placeholder output_vec to contain the true next notes for each song snippet in the batch.\n",
    "'''TODO: Write an expression to obtain the index for the most likely next note predicted by the RNN.'''\n",
    "true_note = tf.argmax(output_vec,1)\n",
    "pred_note = tf.argmax(prediction, 1) # TODO\n",
    "correct_pred = tf.equal(pred_note, true_note)\n",
    "\n",
    "# Next, we obtain a value for the accuracy. \n",
    "# We cast the values in correct_pred to floats, and use tf.reduce_mean\n",
    "#  to figure out the fraction of these values that are 1's (1 = correct, 0 = incorrect)\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the RNN\n",
    "\n",
    "For each training step, we will input a batch of song snippets, and generate the next note for each song snippet in the batch.Loss will be computed at each step too\n",
    "\n",
    "Compared to Tensorflow: Pytorch variables are ready-to-go. If we use TF, we will need to launch a session, initialize all variables first before training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-1-40c3e4a44c2b>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-40c3e4a44c2b>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    if step % display_step == 0 or step == 1:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "    # DISPLAY METRICS\n",
    "    if step % display_step == 0 or step == 1:\n",
    "        # LOSS, ACCURACY: Compute the loss and accuracy by running both operations \n",
    "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict=feed_dict)     \n",
    "        suffix = \"\\nStep \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                 \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                 \"{:.3f}\".format(acc)\n",
    "\n",
    "        print_progress(step, training_steps, barLength=50, suffix=suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a good accuracy level for generating music? An accuracy of 100% means the model has memorized all the songs in the dataset, and can reproduce them at will. An accuracy of 0% means random noise. There must be a happy medium, where the generated music both sounds good and is original. In the words of Gary Marcus, \"Music that is either purely predictable or completely unpredictable is generally considered unpleasant - tedious when it's too predictable, discordant when it's too unpredictable.\" Empirically we've found a good range for this to be `75%` to `90%`, but you should listen to generated output from the next step to see for yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Music Generation\n",
    "The model takes a sequence of notes. We just have to generate one seed note to start iteratively predicting each successive note. It outputs a probability distrubtion over all possible successive notes. We generate an entire song by building up the song length from one seed note. To listen to the demo we write it a file and listen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Pytorch to Tensorflow\n",
    "* In Tensorflow you will have to build the entire graph and then initialize all variables\n",
    "* In Pytorch the graph is built for you while you write Pythonic code. Just call the forward() function\n",
    "* In Tensorflow, we will write all kinds of placeholders\n",
    "* In Pytorch, we can initialize layers and models as a part of a subclass object of nn.Module\n",
    "* Tensorflow math looks more like matrix operations. Its usage is mostly functional based. Will have to call the corresponding function for each step of the operation. \n",
    "* Pytorch is Pythonic, there is the OOP way of defining our model, functional and also sequential way of defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
